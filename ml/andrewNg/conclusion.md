### This is a conclusion about what I have got in the class.

* Week 1 & Week 2
1. There two topics in these two weeks:<br>
> What is ML<br>
> Linear regression<br>
>> Linear regression with one variable<br>
>> Linear regression with multiple variables<br>
>> Normalization<br>
>> Linear regression with Normal Equation<br>
>> Gradient Descent Algorithms:J=J-alpha*(theta derivative of J)<br>
>> Comparison between Gradient Descent Algorithm and Normal Equation<br>
![](https://github.com/edonyM/pyexer/blob/master/ml/andrewNg/pic/comparedtwoalg.PNG)<br>
