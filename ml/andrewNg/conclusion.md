### This is a conclusion about what I have got in the class.

* Week 1 & Week 2
1. There two topics in these two weeks:
        What is ML
        Linear regression |
                          |-- Linear regression with one variable
                          |-- Linear regression with multiple variables
                          |-- Normalization
                          |-- Linear regression with Normal Equation
                          |-- Gradient Descent Algorithms:J=J-alpha*(theta derivative of J)
                          |-- Comparison between Gradient Descent Algorithm and Normal Equation
![](https://github.com/edonyM/pyexer/blob/master/ml/andrewNg/pic/comparedtwoalg.PNG)
